{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Effective TensorFlow for Non-Experts (Google IO '17)](http://youtu.be/?v=5DknTFbcGVM)\n",
    "c:\\Users\\hcche\\Documents\\ML YouTube and Moocs\\Machine Learning\\Misc of ML and AI\\Inception slim models API\\Effective TensorFlow for Non-Experts (Google I_O '17)-5DknTFbcGVM.mp4                      \n",
    "# [Estimator demo using Automobile dataset](https://gist.github.com/martinwicke/6838c23abdc53e6bcda36ed9f40cff39)\n",
    "[Jupyter Notebook martinwicke/automobile.ipynb](https://goo.gl/0OgXiL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[x] 2018.1.3 21:02 沒看懂這個 demo 是要幹嘛? --> 從訓練資料來觀察，看它的 features 跟 labels 就可以猜得出來 --> training_data = df[:160]; training_label = training_data.pop('price') 原來是要猜價錢，哈！see also the video @ 05:30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing to do: Download https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're using pandas to read the CSV file. This is easy for small datasets, but for large and complex datasets,\n",
    "# tensorflow parsing and processing functions are more powerful.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV file does not have a header, so we have to fill in column names.\n",
    "names = [\n",
    "    'symboling', \n",
    "    'normalized-losses', \n",
    "    'make', \n",
    "    'fuel-type', \n",
    "    'aspiration',\n",
    "    'num-of-doors',\n",
    "    'body-style',\n",
    "    'drive-wheels',\n",
    "    'engine-location',\n",
    "    'wheel-base',\n",
    "    'length',\n",
    "    'width',\n",
    "    'height',\n",
    "    'curb-weight',\n",
    "    'engine-type',\n",
    "    'num-of-cylinders',\n",
    "    'engine-size',\n",
    "    'fuel-system',\n",
    "    'bore',\n",
    "    'stroke',\n",
    "    'compression-ratio',\n",
    "    'horsepower',\n",
    "    'peak-rpm',\n",
    "    'city-mpg',\n",
    "    'highway-mpg',\n",
    "    'price',\n",
    "]\n",
    "\n",
    "# We also have to specify dtypes.\n",
    "dtypes = {\n",
    "    'symboling': np.int32, \n",
    "    'normalized-losses': np.float32, \n",
    "    'make': str, \n",
    "    'fuel-type': str, \n",
    "    'aspiration': str,\n",
    "    'num-of-doors': str,\n",
    "    'body-style': str,\n",
    "    'drive-wheels': str,\n",
    "    'engine-location': str,\n",
    "    'wheel-base': np.float32,\n",
    "    'length': np.float32,\n",
    "    'width': np.float32,\n",
    "    'height': np.float32,\n",
    "    'curb-weight': np.float32,\n",
    "    'engine-type': str,\n",
    "    'num-of-cylinders': str,\n",
    "    'engine-size': np.float32,\n",
    "    'fuel-system': str,\n",
    "    'bore': np.float32,\n",
    "    'stroke': np.float32,\n",
    "    'compression-ratio': np.float32,\n",
    "    'horsepower': np.float32,\n",
    "    'peak-rpm': np.float32,\n",
    "    'city-mpg': np.float32,\n",
    "    'highway-mpg': np.float32,\n",
    "    'price': np.float32,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file.\n",
    "df = pd.read_csv('imports-85.data', names=names, dtype=dtypes, na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows don't have price data, we can't use those.\n",
    "df = df.dropna(axis='rows', how='any', subset=['price'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\ 媽呀！下面這段 -- Pandas 太厲害了， df 竟然可以這樣操作。\n",
    "OK __main__ :> float_columns constant float_columns \n",
    "    // ( -- list ) df float_columns\n",
    "OK float_columns . cr\n",
    "['normalized-losses', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-size', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in continuous columns with zeros instead of NaN.\n",
    "float_columns = [k for k,v in dtypes.items() if v == np.float32]\n",
    "df[float_columns] = df[float_columns].fillna(value=0., axis='columns')\n",
    "# Fill missing values in continuous columns with '' instead of NaN (NaN mixed with strings is very bad for us).\n",
    "string_columns = [k for k,v in dtypes.items() if v == str]\n",
    "df[string_columns] = df[string_columns].fillna(value='', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and an eval set.\n",
    "training_data = df[:160]\n",
    "eval_data = df[160:]\n",
    "\n",
    "# Separate input features from labels\n",
    "training_label = training_data.pop('price')\n",
    "eval_label = eval_data.pop('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please make sure that version >= 1.2:\n",
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "# Now we can start using some TensorFlow.\n",
    "import tensorflow as tf\n",
    "print('please make sure that version >= 1.2:')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input function for training: \n",
    "#   num_epochs=None -> will cycle through input data forever\n",
    "#   shuffle=True -> randomize order of input data\n",
    "training_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    x=training_data, y=training_label, \n",
    "    batch_size=64, shuffle=True, num_epochs=None)\n",
    "\n",
    "# Make input function for evaluation:\n",
    "#   shuffle=False -> do not randomize input data\n",
    "eval_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    x=eval_data, y=eval_label, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe how the model should interpret the inputs. \n",
    "# The names of the feature columns have to match the names\n",
    "# of the series in the dataframe.\n",
    "# \n",
    "\n",
    "symboling = tf.feature_column.numeric_column('symboling')\n",
    "normalized_losses = tf.feature_column.numeric_column('normalized-losses')\n",
    "make = tf.feature_column.categorical_column_with_hash_bucket('make', 50)\n",
    "fuel_type = tf.feature_column.categorical_column_with_vocabulary_list('fuel-type', vocabulary_list=['diesel', 'gas'])\n",
    "aspiration = tf.feature_column.categorical_column_with_vocabulary_list('aspiration', vocabulary_list=['std', 'turbo'])\n",
    "num_of_doors = tf.feature_column.categorical_column_with_vocabulary_list('num-of-doors', vocabulary_list=['two', 'four'])\n",
    "body_style = tf.feature_column.categorical_column_with_vocabulary_list('body-style', vocabulary_list=['hardtop', 'wagon', 'sedan', 'hatchback', 'convertible'])\n",
    "drive_wheels = tf.feature_column.categorical_column_with_vocabulary_list('drive-wheels', vocabulary_list=['4wd', 'rwd', 'fwd'])\n",
    "engine_location = tf.feature_column.categorical_column_with_vocabulary_list('engine-location', vocabulary_list=['front', 'rear'])\n",
    "wheel_base = tf.feature_column.numeric_column('wheel-base')\n",
    "length = tf.feature_column.numeric_column('length')\n",
    "width = tf.feature_column.numeric_column('width')\n",
    "height = tf.feature_column.numeric_column('height')\n",
    "curb_weight = tf.feature_column.numeric_column('curb-weight')\n",
    "engine_type = tf.feature_column.categorical_column_with_vocabulary_list('engine-type', ['dohc', 'dohcv', 'l', 'ohc', 'ohcf', 'ohcv', 'rotor'])\n",
    "num_of_cylinders = tf.feature_column.categorical_column_with_vocabulary_list('num-of-cylinders', ['eight', 'five', 'four', 'six', 'three', 'twelve', 'two'])\n",
    "engine_size = tf.feature_column.numeric_column('engine-size')\n",
    "fuel_system = tf.feature_column.categorical_column_with_vocabulary_list('fuel-system', ['1bbl', '2bbl', '4bbl', 'idi', 'mfi', 'mpfi', 'spdi', 'spfi'])\n",
    "bore = tf.feature_column.numeric_column('bore')\n",
    "stroke = tf.feature_column.numeric_column('stroke')\n",
    "compression_ratio = tf.feature_column.numeric_column('compression-ratio')\n",
    "horsepower = tf.feature_column.numeric_column('horsepower')\n",
    "peak_rpm = tf.feature_column.numeric_column('peak-rpm')\n",
    "city_mpg = tf.feature_column.numeric_column('city-mpg')\n",
    "highway_mpg = tf.feature_column.numeric_column('highway-mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_features = [symboling, normalized_losses, make, fuel_type, aspiration, num_of_doors,\n",
    "                   body_style, drive_wheels, engine_location, wheel_base, length, width,\n",
    "                   height, curb_weight, engine_type, num_of_cylinders, engine_size, fuel_system,\n",
    "                   bore, stroke, compression_ratio, horsepower, peak_rpm, city_mpg, highway_mpg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\hcche\\AppData\\Local\\Temp\\tmp3_5drczi\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001635EEBC0B8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\hcche\\\\AppData\\\\Local\\\\Temp\\\\tmp3_5drczi'}\n"
     ]
    }
   ],
   "source": [
    "regressor = tf.contrib.learn.LinearRegressor(feature_columns=linear_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\ 下面查出一堆 classifier, Estimator, Regressor 都不知怎麼區分的？ Stackoverflow 上有人問了也沒人回答 <br>\n",
    "\\ https://stackoverflow.com/questions/43401721/difference-between-dnnlinearcombinedestimator-regressor-classifier-tensorflo<br>\n",
    "\\ 還太新了？<br>\n",
    "\\__main\\__ :> tf.contrib.learn dir . cr \n",
    "\\['BaseEstimator', 'DNNClassifier', 'DNNEstimator', 'DNNLinearCombinedClassifier', 'DNNLinearCombinedEstimator', 'DNNLinearCombinedRegressor', __'DNNRegressor'__, 'DynamicRnnEstimator', 'Estimator', 'Evaluable', 'Experiment', 'ExportStrategy', 'Head', 'InputFnOps', 'KMeansClustering', 'LinearClassifier', 'LinearEstimator', __'LinearRegressor'__, 'LogisticRegressor', 'MetricSpec', 'ModeKeys', 'ModelFnOps', 'NanLossDuringTrainingError', 'NotFittedError', 'PredictionKey', 'ProblemType', 'RunConfig', 'SKCompat', 'SVM', 'TaskType', 'Trainable', ...snip ..., 'binary_svm_head', 'build_parsing_serving_input_fn', 'datasets', 'evaluate', 'extract_dask_data', 'extract_dask_labels', 'extract_pandas_data', 'extract_pandas_labels', 'extract_pandas_matrix', 'graph_actions', 'head', 'infer', 'infer_real_valued_columns_from_input', 'infer_real_valued_columns_from_input_fn', 'io', __'learn_runner'__, 'make_export_strategy', 'models', 'monitors', 'multi_class_head', 'multi_head', 'multi_label_head', 'no_op_train_fn', 'ops', 'poisson_regression_head', 'preprocessing', 'read_batch_examples', 'read_batch_features', 'read_batch_record_features', 'read_keyed_batch_examples', 'read_keyed_batch_examples_shared_queue', 'read_keyed_batch_features', 'read_keyed_batch_features_shared_queue', 'regression_head', 'run_feeds', 'run_n', 'train', 'utils']\n",
    "OK exit\n",
    "OK \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001635573D860>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/output_dir'}\n",
      "WARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From c:\\users\\hcche\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:267: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING:tensorflow:From c:\\users\\hcche\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\linear.py:173: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/output_dir\\model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-05-01:34:16\n",
      "INFO:tensorflow:Restoring parameters from /tmp/output_dir\\model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-05-01:34:22\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 1.43582e+08\n",
      "INFO:tensorflow:Validation (step 1): loss = 1.43582e+08, global_step = 1\n",
      "INFO:tensorflow:loss = 2.28067e+08, step = 1\n",
      "INFO:tensorflow:global_step/sec: 3.26928\n",
      "INFO:tensorflow:loss = 9.12864e+07, step = 101 (8.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.5291\n",
      "INFO:tensorflow:loss = 5.04827e+07, step = 201 (4.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.3171\n",
      "INFO:tensorflow:loss = 6.05508e+07, step = 301 (4.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.3267\n",
      "INFO:tensorflow:loss = 6.48167e+07, step = 401 (4.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.8455\n",
      "INFO:tensorflow:loss = 6.78048e+07, step = 501 (4.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.5558\n",
      "INFO:tensorflow:loss = 2.52829e+07, step = 601 (3.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.9607\n",
      "INFO:tensorflow:loss = 3.58281e+07, step = 701 (3.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.4292\n",
      "INFO:tensorflow:loss = 4.49266e+07, step = 801 (2.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.9142\n",
      "INFO:tensorflow:loss = 3.68768e+07, step = 901 (3.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9021\n",
      "INFO:tensorflow:loss = 4.51669e+07, step = 1001 (3.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.7082\n",
      "INFO:tensorflow:loss = 2.26863e+07, step = 1101 (3.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.7605\n",
      "INFO:tensorflow:loss = 2.91525e+07, step = 1201 (3.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.2311\n",
      "INFO:tensorflow:loss = 4.06984e+07, step = 1301 (3.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.0769\n",
      "INFO:tensorflow:loss = 2.34077e+07, step = 1401 (3.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.2724\n",
      "INFO:tensorflow:loss = 2.71858e+07, step = 1501 (3.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.8243\n",
      "INFO:tensorflow:loss = 2.03435e+07, step = 1601 (2.965 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.329\n",
      "INFO:tensorflow:loss = 3.27622e+07, step = 1701 (3.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.284\n",
      "INFO:tensorflow:loss = 2.0855e+07, step = 1801 (3.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.8127\n",
      "INFO:tensorflow:loss = 3.93939e+07, step = 1901 (4.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.5246\n",
      "INFO:tensorflow:loss = 1.34737e+07, step = 2001 (5.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.0195\n",
      "INFO:tensorflow:loss = 2.59866e+07, step = 2101 (4.747 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.3344\n",
      "INFO:tensorflow:loss = 2.50722e+07, step = 2201 (4.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.5657\n",
      "INFO:tensorflow:loss = 4.33049e+07, step = 2301 (3.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.4564\n",
      "INFO:tensorflow:loss = 2.0973e+07, step = 2401 (3.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.6007\n",
      "INFO:tensorflow:loss = 2.06943e+07, step = 2501 (4.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.6668\n",
      "INFO:tensorflow:loss = 3.18715e+07, step = 2601 (3.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.258\n",
      "INFO:tensorflow:loss = 1.61522e+07, step = 2701 (3.004 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.4421\n",
      "INFO:tensorflow:loss = 2.05603e+07, step = 2801 (3.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.2802\n",
      "INFO:tensorflow:loss = 2.86036e+07, step = 2901 (3.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.8406\n",
      "INFO:tensorflow:loss = 2.6636e+07, step = 3001 (3.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.5909\n",
      "INFO:tensorflow:loss = 1.30141e+07, step = 3101 (4.501 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.6209\n",
      "INFO:tensorflow:loss = 2.82921e+07, step = 3201 (4.004 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.2729\n",
      "INFO:tensorflow:loss = 3.64597e+07, step = 3301 (4.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.9998\n",
      "INFO:tensorflow:loss = 1.99498e+07, step = 3401 (4.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.0241\n",
      "INFO:tensorflow:loss = 2.3769e+07, step = 3501 (3.979 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.5219\n",
      "INFO:tensorflow:loss = 2.07359e+07, step = 3601 (2.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.4742\n",
      "INFO:tensorflow:loss = 3.74962e+07, step = 3701 (3.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.0552\n",
      "INFO:tensorflow:loss = 2.88289e+07, step = 3801 (3.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.3367\n",
      "INFO:tensorflow:loss = 2.4083e+07, step = 3901 (3.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.7411\n",
      "INFO:tensorflow:loss = 2.60521e+07, step = 4001 (3.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.1929\n",
      "INFO:tensorflow:loss = 2.54033e+07, step = 4101 (4.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.7417\n",
      "INFO:tensorflow:loss = 1.95564e+07, step = 4201 (4.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.2478\n",
      "INFO:tensorflow:loss = 3.28909e+07, step = 4301 (6.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.5472\n",
      "INFO:tensorflow:loss = 3.10571e+07, step = 4401 (3.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.2623\n",
      "INFO:tensorflow:loss = 1.89955e+07, step = 4501 (3.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.8693\n",
      "INFO:tensorflow:loss = 2.29422e+07, step = 4601 (3.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.5189\n",
      "INFO:tensorflow:loss = 2.65752e+07, step = 4701 (2.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.3604\n",
      "INFO:tensorflow:loss = 2.4824e+07, step = 4801 (2.995 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.3177\n",
      "INFO:tensorflow:loss = 2.17156e+07, step = 4901 (3.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.9557\n",
      "INFO:tensorflow:loss = 1.96242e+07, step = 5001 (3.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.5767\n",
      "INFO:tensorflow:loss = 1.83966e+07, step = 5101 (4.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0045\n",
      "INFO:tensorflow:loss = 2.73211e+07, step = 5201 (4.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.8411\n",
      "INFO:tensorflow:loss = 2.43012e+07, step = 5301 (4.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.2916\n",
      "INFO:tensorflow:loss = 2.31459e+07, step = 5401 (3.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.6936\n",
      "INFO:tensorflow:loss = 2.09588e+07, step = 5501 (3.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.0322\n",
      "INFO:tensorflow:loss = 1.97832e+07, step = 5601 (2.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.7835\n",
      "INFO:tensorflow:loss = 2.2309e+07, step = 5701 (3.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.7205\n",
      "INFO:tensorflow:loss = 2.87568e+07, step = 5801 (3.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.226\n",
      "INFO:tensorflow:loss = 1.56553e+07, step = 5901 (3.010 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 33.6781\n",
      "INFO:tensorflow:loss = 1.82941e+07, step = 6001 (2.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.0922\n",
      "INFO:tensorflow:loss = 2.16285e+07, step = 6101 (2.933 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.5035\n",
      "INFO:tensorflow:loss = 1.7377e+07, step = 6201 (3.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.7818\n",
      "INFO:tensorflow:loss = 2.22345e+07, step = 6301 (3.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.2252\n",
      "INFO:tensorflow:loss = 1.99859e+07, step = 6401 (3.820 sec)\n",
      "INFO:tensorflow:global_step/sec: 17.8684\n",
      "INFO:tensorflow:loss = 7.55601e+06, step = 6501 (5.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.0052\n",
      "INFO:tensorflow:loss = 2.33266e+07, step = 6601 (4.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.7754\n",
      "INFO:tensorflow:loss = 2.87699e+07, step = 6701 (2.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.416\n",
      "INFO:tensorflow:loss = 2.73927e+07, step = 6801 (0.876 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.657\n",
      "INFO:tensorflow:loss = 3.03666e+07, step = 6901 (0.765 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.565\n",
      "INFO:tensorflow:loss = 1.63938e+07, step = 7001 (0.750 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.917\n",
      "INFO:tensorflow:loss = 2.11068e+07, step = 7101 (0.776 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.482\n",
      "INFO:tensorflow:loss = 2.12668e+07, step = 7201 (0.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.364\n",
      "INFO:tensorflow:loss = 1.28923e+07, step = 7301 (0.739 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.622\n",
      "INFO:tensorflow:loss = 2.05616e+07, step = 7401 (0.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.351\n",
      "INFO:tensorflow:loss = 2.38121e+07, step = 7501 (0.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.124\n",
      "INFO:tensorflow:loss = 2.13792e+07, step = 7601 (0.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.356\n",
      "INFO:tensorflow:loss = 1.09277e+07, step = 7701 (0.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.604\n",
      "INFO:tensorflow:loss = 2.16912e+07, step = 7801 (0.768 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.066\n",
      "INFO:tensorflow:loss = 2.39581e+07, step = 7901 (0.769 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.993\n",
      "INFO:tensorflow:loss = 1.99566e+07, step = 8001 (0.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.647\n",
      "INFO:tensorflow:loss = 1.6727e+07, step = 8101 (0.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.08\n",
      "INFO:tensorflow:loss = 1.69934e+07, step = 8201 (0.774 sec)\n",
      "INFO:tensorflow:global_step/sec: 134.587\n",
      "INFO:tensorflow:loss = 2.12338e+07, step = 8301 (0.743 sec)\n",
      "INFO:tensorflow:global_step/sec: 123.092\n",
      "INFO:tensorflow:loss = 1.86405e+07, step = 8401 (0.812 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.144\n",
      "INFO:tensorflow:loss = 3.30084e+07, step = 8501 (0.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.083\n",
      "INFO:tensorflow:loss = 2.26136e+07, step = 8601 (0.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.569\n",
      "INFO:tensorflow:loss = 1.94337e+07, step = 8701 (0.747 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.137\n",
      "INFO:tensorflow:loss = 1.21972e+07, step = 8801 (0.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.263\n",
      "INFO:tensorflow:loss = 2.16311e+07, step = 8901 (0.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.467\n",
      "INFO:tensorflow:loss = 1.84595e+07, step = 9001 (0.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.861\n",
      "INFO:tensorflow:loss = 2.17182e+07, step = 9101 (0.758 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.2292\n",
      "INFO:tensorflow:loss = 1.71634e+07, step = 9201 (1.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.7577\n",
      "INFO:tensorflow:loss = 1.9179e+07, step = 9301 (1.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.8219\n",
      "INFO:tensorflow:loss = 1.51944e+07, step = 9401 (1.730 sec)\n",
      "INFO:tensorflow:global_step/sec: 56.7224\n",
      "INFO:tensorflow:loss = 1.87096e+07, step = 9501 (1.758 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.8336\n",
      "INFO:tensorflow:loss = 1.44282e+07, step = 9601 (1.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6401\n",
      "INFO:tensorflow:loss = 2.01117e+07, step = 9701 (2.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.8555\n",
      "INFO:tensorflow:loss = 1.32946e+07, step = 9801 (2.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.847\n",
      "INFO:tensorflow:loss = 2.06791e+07, step = 9901 (2.238 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/output_dir\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.74607e+07.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-05-01:39:21\n",
      "INFO:tensorflow:Restoring parameters from /tmp/output_dir\\model.ckpt-10000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-05-01:39:24\n",
      "INFO:tensorflow:Saving dict for global step 10000: global_step = 10000, loss = 8.22466e+06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'global_step': 10000, 'loss': 8224656.5}, [])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def experiment_fn(run_config, params):\n",
    "  # This function makes an Experiment, containing an Estimator and inputs for training and evaluation.\n",
    "  # You can use params and config here to customize the Estimator depending on the cluster or to use\n",
    "  # hyperparameter tuning.\n",
    "\n",
    "  # Collect information for training\n",
    "  return tf.contrib.learn.Experiment(estimator=tf.contrib.learn.LinearRegressor(\n",
    "                                     feature_columns=linear_features, config=run_config),\n",
    "                                     train_input_fn=training_input_fn,\n",
    "                                     train_steps=10000,\n",
    "                                     eval_input_fn=eval_input_fn)\n",
    "# shutil http://www.cnblogs.com/CLTANG/archive/2011/11/15/2249257.html\n",
    "import shutil\n",
    "shutil.rmtree(\"/tmp/output_dir\", ignore_errors=True)  # 刪除暫存目錄\n",
    "tf.contrib.learn.learn_runner.run(experiment_fn, run_config=tf.contrib.learn.RunConfig(model_dir=\"/tmp/output_dir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(input_fn=training_input_fn, steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.evaluate(input_fn=eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_features = [\n",
    "    #numerical features\n",
    "    symboling, normalized_losses, wheel_base, length, width, height, curb_weight, engine_size,\n",
    "    bore, stroke, compression_ratio, horsepower, peak_rpm, city_mpg, highway_mpg,    \n",
    "    # densify categorical features:\n",
    "    tf.feature_column.indicator_column(make),\n",
    "    tf.feature_column.indicator_column(fuel_type),\n",
    "    tf.feature_column.indicator_column(aspiration),\n",
    "    tf.feature_column.indicator_column(num_of_doors),\n",
    "    tf.feature_column.indicator_column(body_style),\n",
    "    tf.feature_column.indicator_column(drive_wheels), \n",
    "    tf.feature_column.indicator_column(engine_location),\n",
    "    tf.feature_column.indicator_column(engine_type),\n",
    "    tf.feature_column.indicator_column(num_of_cylinders),\n",
    "    tf.feature_column.indicator_column(fuel_system),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnregressor = tf.contrib.learn.DNNRegressor(feature_columns=dnn_features, hidden_units=[50, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnregressor.fit(input_fn=training_input_fn, steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnregressor.evaluate(input_fn=eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_fn(run_config, params):\n",
    "  # This function makes an Experiment, containing an Estimator and inputs for training and evaluation.\n",
    "  # You can use params and config here to customize the Estimator depending on the cluster or to use\n",
    "  # hyperparameter tuning.\n",
    "\n",
    "  # Collect information for training\n",
    "  return tf.contrib.learn.Experiment(estimator=tf.contrib.learn.LinearRegressor(\n",
    "                                     feature_columns=linear_features, config=run_config),\n",
    "                                     train_input_fn=training_input_fn,\n",
    "                                     train_steps=10000,\n",
    "                                     eval_input_fn=eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil http://www.cnblogs.com/CLTANG/archive/2011/11/15/2249257.html\n",
    "import shutil\n",
    "shutil.rmtree(\"/tmp/output_dir\", ignore_errors=True)  # 刪除暫存目錄\n",
    "tf.contrib.learn.learn_runner.run(experiment_fn, run_config=tf.contrib.learn.RunConfig(model_dir=\"/tmp/output_dir\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "__main__ :> df constant df // ( -- dataframe ) \n",
    "__main__ :> pd constant pd // ( -- Pandas )\n",
    "\n",
    "df :> [:3] . cr\n",
    "   symboling  normalized-losses         make fuel-type aspiration  \\\n",
    "0          3                0.0  alfa-romero       gas        std   \n",
    "1          3                0.0  alfa-romero       gas        std   \n",
    "2          1                0.0  alfa-romero       gas        std   \n",
    "\n",
    "  num-of-doors   body-style drive-wheels engine-location  wheel-base   ...     \\\n",
    "0          two  convertible          rwd           front   88.599998   ...      \n",
    "1          two  convertible          rwd           front   88.599998   ...      \n",
    "2          two    hatchback          rwd           front   94.500000   ...      \n",
    "\n",
    "   engine-size  fuel-system  bore  stroke compression-ratio horsepower  \\\n",
    "0        130.0         mpfi  3.47    2.68               9.0      111.0   \n",
    "1        130.0         mpfi  3.47    2.68               9.0      111.0   \n",
    "2        152.0         mpfi  2.68    3.47               9.0      154.0   \n",
    "\n",
    "   peak-rpm city-mpg  highway-mpg    price  \n",
    "0    5000.0     21.0         27.0  13495.0  \n",
    "1    5000.0     21.0         27.0  16500.0  \n",
    "2    5000.0     19.0         26.0  16500.0  \n",
    "\n",
    "[3 rows x 26 columns]\n",
    "\n",
    "\\ Pandas 的 DataFrame 功能超多\n",
    "OK df dir . cr\n",
    "[...snip..., 'abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'append', 'apply', 'applymap', 'as_blocks', 'as_matrix', 'asfreq', 'asof', 'aspiration', 'assign', 'astype', 'at', 'at_time', 'axes', 'between_time', 'bfill', 'blocks', 'bool', 'bore', 'boxplot', 'clip', 'clip_lower', 'clip_upper', 'columns', 'combine', 'combine_first', 'compound', 'consolidate', 'convert_objects', 'copy', 'corr', 'corrwith', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'dropna', 'dtypes', 'duplicated', 'empty', 'eq', 'equals', 'eval', 'ewm', 'expanding', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'floordiv', 'from_csv', 'from_dict', 'from_items', 'from_records', 'ftypes', 'ge', 'get', 'get_dtype_counts', 'get_ftype_counts', 'get_value', 'get_values', 'groupby', 'gt', 'head', 'height', 'hist', 'horsepower', 'iat', 'idxmax', 'idxmin', 'iloc', 'index', 'info', 'insert', 'interpolate', 'is_copy', 'isin', 'isnull', 'items', 'iteritems', 'iterrows', 'itertuples', 'ix', 'join', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'length', 'loc', 'lookup', 'lt', 'mad', 'make', 'mask', 'max', 'mean', 'median', 'melt', 'memory_usage', 'merge', 'min', 'mod', 'mode', 'mul', 'multiply', 'ndim', 'ne', 'nlargest', 'notnull', 'nsmallest', 'nunique', 'pct_change', 'pipe', 'pivot', 'pivot_table', 'plot', 'pop', 'pow', 'price', 'prod', 'product', 'quantile', 'query', 'radd', 'rank', 'rdiv', 'reindex', 'reindex_axis', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'replace', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'select', 'select_dtypes', 'sem', 'set_axis', 'set_index', 'set_value', 'shape', 'shift', 'size', 'skew', 'slice_shift', 'sort_index', 'sort_values', 'sortlevel', 'squeeze', 'stack', 'std', 'stroke', 'style', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'symboling', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dense', 'to_dict', 'to_excel', 'to_feather', 'to_gbq', 'to_hdf', 'to_html', 'to_json', 'to_latex', 'to_msgpack', 'to_panel', 'to_period', 'to_pickle', 'to_records', 'to_sparse', 'to_sql', 'to_stata', 'to_string', 'to_timestamp', 'to_xarray', 'transform', 'transpose', 'truediv', 'truncate', 'tshift', 'tz_convert', 'tz_localize', 'unstack', 'update', 'values', 'var', 'where', 'width', 'xs']\n",
    "\\ 試著把 dataFrame 存放成 excel 用來觀察它\n",
    "OK df :> to_excel py: help(pop()) \\ 它要求 excel writer 搞不定\n",
    "OK df :> to_csv(\"imports-85.csv\") \\ csv 很簡單，一下就成功了。\n",
    "\n",
    "__main__ :> names constant names // ( -- list ) dataFrame column names\n",
    "__main__ :> dtypes constant dtypes // ( -- dict ) dataFrame column dtypes\n",
    "__main__ :> names . cr\n",
    "['symboling', 'normalized-losses', 'make', ..snip..., 'price']\n",
    "\n",
    "OK __main__ :> dtypes . cr\n",
    "{'symboling': <class 'numpy.int32'>, 'normalized-losses': <class 'numpy.float32'>, 'make': <class 'str'>, 'fuel-type': <class 'str'>, 'aspiration': <class 'str'>, 'num-of-doors': <class 'str'>, 'body-style': <class 'str'>, 'drive-wheels': <class 'str'>, 'engine-location': <class 'str'>, 'wheel-base': <class 'numpy.float32'>, 'length': <class 'numpy.float32'>, 'width': <class 'numpy.float32'>, 'height': <class 'numpy.float32'>, 'curb-weight': <class 'numpy.float32'>, 'engine-type': <class 'str'>, 'num-of-cylinders': <class 'str'>, 'engine-size': <class 'numpy.float32'>, 'fuel-system': <class 'str'>, 'bore': <class 'numpy.float32'>, 'stroke': <class 'numpy.float32'>, 'compression-ratio': <class 'numpy.float32'>, 'horsepower': <class 'numpy.float32'>, 'peak-rpm': <class 'numpy.float32'>, 'city-mpg': <class 'numpy.float32'>, 'highway-mpg': <class 'numpy.float32'>, 'price': <class 'numpy.float32'>}\n",
    "\n",
    "# Fill missing values in continuous columns with zeros instead of NaN.\n",
    "float_columns = [k for k,v in dtypes.items() if v == np.float32]\n",
    "df[float_columns] = df[float_columns].fillna(value=0., axis='columns')\n",
    "\n",
    "# Fill missing values in continuous columns with '' instead of NaN (NaN mixed with strings is very bad for us).\n",
    "string_columns = [k for k,v in dtypes.items() if v == str]\n",
    "df[string_columns] = df[string_columns].fillna(value='', axis='columns')\n",
    "\n",
    "OK __main__ :> float_columns constant float_columns \n",
    "    // ( -- list ) df float_columns\n",
    "OK float_columns . cr\n",
    "['normalized-losses', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-size', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
    "OK \n",
    "\n",
    "__main__ :> dnnregressor constant dnnregressor // ( -- obj ) \n",
    "OK dnnregressor . cr\n",
    "DNNRegressor(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._RegressionHead object at 0x000001E7E70A1748>, 'hidden_units': [50, 30, 10], 'feature_columns': (_NumericColum ... snip .... \n",
    "\n",
    "dnnregressor dir . cr\n",
    "['_Config', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__metaclass__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_call_model_fn', '_check_inputs', '_config', '_device_fn', '_evaluate_model', '_extract_metric_update_ops', '_feature_columns', '_feature_engineering_fn', '_features_info', '_filter_predictions', '_get_eval_ops', '_get_feature_ops_from_example', '_get_features_from_input_fn', '_get_predict_ops', '_get_train_ops', '_graph', '_infer_model', '_is_input_constant', '_labels_info', '_model_dir', '_model_fn', '_predict_generator', '_session_config', '_train_model', 'config', 'evaluate', 'export', 'export_savedmodel', 'fit', 'get_params', 'get_variable_names', 'get_variable_value', 'model_dir', 'params', 'partial_fit', 'predict', 'predict_scores', 'set_params']\n",
    "OK \n",
    "\n",
    "\\ 抓出一筆 feature \n",
    "OK eval_data :> [10:11] . cr\n",
    "     symboling  normalized-losses    make fuel-type aspiration num-of-doors  \\\n",
    "174         -1               65.0  toyota    diesel      turbo         four   \n",
    "\n",
    "    body-style drive-wheels engine-location  wheel-base     ...       \\\n",
    "174      sedan          fwd           front  102.400002     ...        \n",
    "\n",
    "     num-of-cylinders  engine-size  fuel-system  bore stroke  \\\n",
    "174              four        110.0          idi  3.27   3.35   \n",
    "\n",
    "    compression-ratio  horsepower peak-rpm  city-mpg  highway-mpg  \n",
    "174              22.5        73.0   4500.0      30.0         33.0  \n",
    "\n",
    "[1 rows x 25 columns]\n",
    "OK \n",
    "dnnregressor :> predict(v('eval_data')[0:1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "peforth.ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dnnregressor.predict(x=dict(eval_data[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dnnregressor.predict(x=dict(eval_data[20:21]))\n",
    "print(eval_data[20:21])\n",
    "[i for i in _]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dnnregressor.predict(x=dict(eval_data[27:28]))\n",
    "print(eval_data[27:28])\n",
    "[i for i in _]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[k for k,v in dtypes if v == np.float32]\n",
    "mydict = {'aa':11,'bb':22}\n",
    "[i for i in mydict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mydict[i] for i in mydict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(n,k) for k,n in mydict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = tf.contrib.learn.LinearRegressor(feature_columns=linear_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peforth.ok()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
